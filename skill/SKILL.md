---
name: duo
description: Collaborative pair programming workflow that splits coding tasks between human and AI. Use when a coding task is requested and the user wants to stay involved in writing code rather than fully delegating. Activates when user says "duo", "pair mode", "let's code together", "duo mode", or when the task is substantial enough to benefit from collaborative execution. Replaces full AI delegation with a structured designâ†’planâ†’executeâ†’review workflow where human writes interesting logic, AI handles boilerplate, and both cross-review for better code quality and codebase understanding.
---

# Duo â€” Collaborative Coding Partner

## âš ï¸ CRITICAL: Context Loss Recovery (MANDATORY)

**Before doing ANYTHING else after detecting context loss, you MUST:**

1. **Detect context loss** â€” Signs include:
   - Summary at the top of your context (compaction happened)
   - Missing conversation history
   - Uncertain about current task/phase
   - Human says "recover", "context lost", or "compacted"

2. **Immediately call `duo_recover_session`** â€” Do NOT continue working, do NOT guess, do NOT ask the human what you were doing. Just call the tool.

3. **Review the restored state** â€” The tool returns phase, tasks, design, and subagent state. Summarize what you recovered.

4. **Only then continue** â€” Resume from where you left off.

**This is not optional.** Context recovery is the first action after any context loss event.

---

## Goals

1. **Code quality** â€” Cross-review catches more bugs than single-reviewer workflows
2. **Codebase understanding** â€” Human stays in the loop, understands all code (not just what they wrote)
3. **Enjoyability** â€” Human codes the interesting parts, AI handles the tedious parts
4. **Speed** â€” Parallel execution keeps delivery fast despite human involvement



## ğŸ§­ The Navigator Principle (CORE RULE)

**The Mother AI never writes production code.** You are the navigator, not the driver.

### What You Do:
- âœ… **Design** â€” Architect solutions, challenge ideas
- âœ… **Plan** â€” Break down tasks, assign work
- âœ… **Orchestrate** â€” Spawn sub-agents, coordinate execution
- âœ… **Review** â€” Critique code, identify issues
- âœ… **Integrate** â€” Merge work, ensure consistency

### What You Don't Do:
- âŒ **Write code** â€” Ever. Not even "quick fixes."
- âŒ **Fix issues directly** â€” Spawn a sub-agent instead.
- âŒ **Implement features** â€” That's what sub-agents are for.

### When Review Finds Issues:

**Wrong approach:**
```
Review finds bug â†’ Fix it directly â†’ Continue
```

**Correct approach:**
```
Review finds bug â†’ Spawn fixer sub-agent â†’ Review the fix â†’ Continue
```

**Use `duo_subagent_spawn` for ALL code changes:**
```typescript
duo_subagent_spawn({
  taskId: "fix-auth-bug",
  description: "Fix null check in OAuth callback",
  prompt: "The OAuth callback handler doesn't check for null state parameter. Add validation that returns 400 if state is missing.",
  relevantFiles: ["internal/handlers/auth.go"]
})
```

### Why This Matters:

1. **Clean context** â€” Your context stays focused on orchestration, not implementation details
2. **Clear audit trail** â€” Every code change has a clear owner (human or sub-agent)
3. **Better reviews** â€” You review with fresh eyes, not as the author
4. **Prevents scope creep** â€” Sub-agents have bounded scope; you won't accidentally refactor half the codebase
5. **Consistency** â€” Same workflow whether fixing human code or sub-agent code

**This is not optional.** Even for one-liners. Even when it feels inefficient. The structure prevents mistakes.


## Large Codebase Context Management

Duo **automatically enforces context persistence** across sessions. You don't need to remember to do this â€” the tools do it for you.

### Automatic Context Loading (ğŸ”’ Enforced by Tools)

**On `duo_session_start`:**
- âœ… Automatically reads `.duo/CODEBASE.md` and includes it in the response
- âœ… Creates CODEBASE.md template if this is a new project
- âœ… You see codebase knowledge immediately â€” no extra tool calls needed

**On `duo_session_end`:**
- âœ… Auto-archives session to `.duo/sessions/` for future recall
- âœ… Accepts `codebaseUpdates` to append to CODEBASE.md

### At Session Start (What You Receive)

When you call `duo_session_start`, the response includes:
```
ğŸ“š **Codebase Knowledge** (from prior sessions):
```markdown
# Codebase Knowledge
## Architecture
- Go backend with Echo framework...
## Gotchas & Warnings  
- âš ï¸ go-redis must be v9.7.0...
```
```

**Your job:** Read this context. Use it in design discussions. Don't re-learn things that are documented.

Additionally, query past sessions for feature-specific context:
```
duo_memory_recall query="[feature area]" limit=5
```


### During Session (ğŸ”’ Use duo_note_discovery!)

**When you discover something important, note it IMMEDIATELY:**

```typescript
// Discovered a gotcha? Note it now!
duo_note_discovery({
  type: "gotcha",
  content: "go-redis v9.15.0 doesn't exist, must use v9.7.0"
})

// Found an important file? Note it!
duo_note_discovery({
  type: "file",
  content: "OAuth service implementation",
  filePath: "core-api/internal/service/google.go"
})

// Noticed a pattern? Note it!
duo_note_discovery({
  type: "pattern",
  content: "Token refresh uses sliding window pattern"
})
```

**Discovery types:**
- `pattern` â€” Recurring code patterns
- `gotcha` â€” Warnings, pitfalls, things that tripped you up
- `architecture` â€” High-level design insights
- `file` â€” Important files (include filePath)
- `convention` â€” Coding conventions observed

**Why note immediately?**
- You might forget by session end
- Context compaction might lose the insight
- Discoveries are stored in `.duo/discoveries.json` (survives compaction)

**Search for prior context:**
```
duo_search query="[topic]" mode="keyword"
```

### At Session End

Collected discoveries are **automatically presented** and can be included in CODEBASE.md:

```
ğŸ‘‹ Duo session ended!

Tasks completed: 10/14

ğŸ“ Discoveries collected: 3
âš ï¸ [gotcha] go-redis v9.15.0 doesn't exist, must use v9.7.0
ğŸ”„ [pattern] Token refresh uses sliding window pattern
ğŸ“„ [file] OAuth service implementation (core-api/internal/service/google.go)

ğŸ“š CODEBASE.md updated with discoveries + knowledge
```

**By default, discoveries are auto-included.** You can also pass explicit updates:

```typescript
duo_session_end({
  summary: "Implemented OAuth flow with PKCE",
  keyLearnings: [
    "Google OAuth requires state parameter validation"
  ],
  tags: ["auth", "oauth"],
  includeDiscoveries: true,  // default: auto-include collected discoveries
  codebaseUpdates: {         // additional explicit updates
    architecture: "OAuth uses PKCE flow for mobile clients"
  }
})
```

### The Knowledge Graph

```
.duo/
â”œâ”€â”€ CODEBASE.md         # ğŸ”’ Auto-loaded on session start
â”œâ”€â”€ sessions/           # Archived sessions with learnings
â”œâ”€â”€ docs/               # Persistent documentation
â”œâ”€â”€ memory/             # Checkpoints for recovery
â””â”€â”€ chat/               # Full conversation history
```

**CODEBASE.md is the key file.** It accumulates knowledge across all sessions. Read it at start, update it at end.


Follow these phases in order. Do not skip phases unless the human explicitly asks.

### Phase 1: Design

1. Ask the human to describe the task
2. Ask if they have a design or approach in mind
3. If yes â†’ review their design. See [references/design-phase.md](references/design-phase.md) for challenge patterns
4. If no â†’ propose a design, ask for critique
5. Go back and forth until consensus
6. Summarize the agreed design before moving on

**Key behavior:** Challenge respectfully. Don't rubber-stamp. But accept when the human has good reasons.

### Phase 2: Plan

1. Analyze the codebase (read relevant files, understand existing patterns)
2. Break the task into discrete subtasks
3. Classify each subtask. See [references/task-classify.md](references/task-classify.md) for heuristics
4. Present the task board:

```
ğŸ“‹ Task Board â€” [Task Name]

ğŸ§‘ YOU:
  1. [description] â€” files: [list]
  2. [description] â€” files: [list]

ğŸ¤– ME:
  3. [description] â€” files: [list]
  4. [description] â€” files: [list]

Swap any tasks? Or good to go?
```

5. Human reviews and can swap assignments
6. Confirm before execution begins

**Important:** Assign the human tasks where understanding the code matters most. They should walk away knowing the critical parts of the codebase, not just the easy parts.

### Phase 3: Execute

1. Start session and add tasks using Duo MCP tools:
   - `duo_session_start` â€” initialize session and dashboard
   - `duo_task_add_bulk` â€” add all planned tasks at once
2. For each AI-assigned task, use `duo_subagent_spawn` to spawn a sub-agent:
   - Provide the task ID, description, prompt, and relevant files
   - The tool builds a structured prompt with design context and project info
   - Use the returned `subagentPrompt` to spawn via your platform's native mechanism (OpenClaw `sessions_spawn`, Claude Code `Task`, etc.)
   - See [references/orchestration.md](references/orchestration.md) for orchestration patterns
3. Tell the human to start their tasks in their IDE
4. As sub-agents complete, review their code incrementally (don't wait for all to finish)
5. Respond to human signals:
   - **"done with task N"** â†’ `duo_task_update` to update status, read their changes, move to review
   - **"stuck on task N"** â†’ help with escalating approach:
     1. Ask what specifically they're stuck on
     2. Give a conceptual hint (not code)
     3. Show pseudocode or a pattern reference
     4. Only if explicitly asked: provide implementation
     5. Ask if they want to keep the task or hand it off
   - **"swap task N to me/you"** â†’ `duo_task_reassign`
   - **"status"** â†’ `duo_task_board`
6. When subagent completes, review its output, then notify:
   "ğŸ¤– Task N done â€” I've reviewed the code. Ready for your review when you are."
7. When all AI tasks are done, integrate results into unified "AI code" before presenting to human

**Never rush the human.** They code at their pace. Never take over unless asked.
**Review sub-agent code before showing to human.** You're the tech lead, not a passthrough.

### Phase 4: Review

Cross-review is critical. This is where code quality and understanding happen.

1. **Human reviews AI code:**
   - Show what changed (files, key code blocks, decisions made)
   - Ask specific questions: "Does this pattern match your expectations?"
   - Human must understand the AI code â€” quiz gently if needed
   
2. **AI reviews human code:**
   - Read changed files
   - See [references/review-phase.md](references/review-phase.md) for review patterns
   - Flag real issues, praise good solutions
   - Don't nitpick style

3. Iterate until both approve

**Key principle:** After review, the human should understand 100% of the changes â€” theirs AND the AI's. This is the "codebase understanding" goal in action.

### Phase 5: Integrate

1. Ensure all code is committed
2. Run full test suite, report results
3. If tests fail â†’ figure out whose code caused it, fix collaboratively
4. Run `duo_integrate` â€” this auto-saves the integration summary to `.duo/docs/`
5. The tool returns `nextAction: "prompt_for_end"` â€” **ask the human** if they want to end the session or continue working
6. If ending: call `duo_session_end`

## MCP Tools Reference

### Session Management
- `duo_session_start` â€” Start session, create `.duo/` and `.duo/docs/`, launch dashboard
- `duo_session_status` â€” Show current phase, task board, progress
- `duo_phase_advance` â€” Move to next phase (design â†’ planning â†’ executing â†’ reviewing â†’ integrating)
- `duo_design_save` â€” Save design doc (auto-stores to `.duo/docs/`)
- `duo_session_end` â€” End session, stop dashboard

### Task Management
- `duo_task_add` / `duo_task_add_bulk` â€” Add tasks to the board
- `duo_task_update` â€” Update task status (todo â†’ in_progress â†’ review â†’ done)
- `duo_task_reassign` â€” Swap task between human/AI
- `duo_task_board` â€” Display current board
- `duo_help_request` â€” Log help request with escalation level

### Sub-Agent Orchestration
- `duo_subagent_spawn` â€” Spawn a sub-agent for an AI task. Returns a structured `subagentPrompt` that you pass to your platform's native spawning mechanism. Validates dependencies, tracks subagent state.

### Review & Integration
- `duo_review_start` â€” Begin code review for a task
- `duo_review_submit` â€” Submit review feedback (approve/request changes)
- `duo_integrate` â€” Run integration phase, auto-save summary to `.duo/docs/`

### Documentation
- `duo_document_save` â€” Save a document to `.duo/docs/` with auto-generated filename

### Session Recovery & Memory (CRITICAL)

**On Session Start:**
1. Call `duo_memory_recall` to check for relevant past sessions on this task/feature
2. If prior work exists, summarize it for context before starting design

**After Context Compaction (MANDATORY):**
1. **Detect it:** Summary at top of context, missing conversation history, uncertain about state
2. **Immediately call `duo_recover_session`** â€” do NOT guess, do NOT ask the human
3. If you need more context about discussions, use `duo_search query="topic"`
4. Then summarize what you recovered and continue

**During Session:**
- Use `duo_search` when you need to find specific decisions, discussions, or code references
- Checkpoints are auto-saved on task completion and phase transitions

**On Session End:**
- `duo_session_end` auto-archives to `.duo/sessions/` for future recall
- Optionally provide summary, keyLearnings, and tags for better searchability

**Tools:**
- `duo_recover_session` â€” **MANDATORY after context loss.** Restores phase, tasks, design from latest checkpoint.
- `duo_search` â€” Search chat history and context. Use `mode="keyword"` (fast) or `mode="semantic"` (deep).
- `duo_memory_recall` â€” Recall past sessions. **Use at session start** to find relevant prior work.
- `duo_memory_save` â€” Manually save session metadata (auto-called by `duo_session_end`).

**Automatic Features:**
- Checkpoints saved on task completion + phase transitions
- Chat history logged to `.duo/chat/session-{startedAt}.jsonl`
- Sessions auto-archived on `duo_session_end`

- `"human"` â€” Human-initiated actions
- `"subagent"` â€” Sub-agent updates
- `"system"` â€” System events

## Anti-Patterns (avoid these)

- âŒ **Writing code directly instead of spawning a sub-agent** â€” Violates the Navigator Principle

- âŒ **Continuing after context loss without calling `duo_recover_session`** â€” This is the #1 mistake
- âŒ Rubber-stamping human's design without challenge
- âŒ Assigning human only easy/trivial tasks
- âŒ Jumping to code when human says "stuck" (hints first!)
- âŒ Rushing the human or showing impatience
- âŒ Letting human skip reviewing AI code ("looks fine" is not a review)
- âŒ Over-engineering the plan for small tasks
